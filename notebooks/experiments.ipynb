{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM6fmwvaLnlsA3u/HVv9bqu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Suraj-Sedai/minigpt-inference/blob/main/notebooks/experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MiniGPT-Inference [KV-CACHE TRANSFORMER]\n",
        "\n",
        "**A From-Scratch Transformer Inference Engine**\n",
        "\n",
        "MiniGPT-Inference is a from-scratch, production-grade Transformer inference engine designed to execute autoregressive decoding efficiently using Keyâ€“Value (KV) caching, incremental decoding, and batched generation.\n",
        "\n",
        "Unlike training-focused implementations, this project centers on inference-time systems engineering, emphasizing:\n",
        "- Computational complexity reduction\n",
        "- Memory efficiency\n",
        "- Deterministic correctness\n",
        "- Measurable performance gains\n",
        "\n",
        "The system is architected to reflect how modern large language models (LLMs) are served in real-world environments."
      ],
      "metadata": {
        "id": "bq03RwLpRE-9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48b947b0"
      },
      "source": [
        "# Project Setup: Model Architecture Configuration\n",
        "\n",
        "This section outlines the foundational configuration for our model. The `ModelConfig` dataclass is used to define key architectural hyperparameters, centralizing them for clarity, reusability, and ease of modification.\n",
        "\n",
        "The parameters included in `ModelConfig` are typically found in transformer-based models and include:\n",
        "*   `vocab_size`: The size of the vocabulary, representing the number of unique tokens the model can process.\n",
        "*   `n_layers`: The number of transformer layers or blocks within the model's architecture.\n",
        "*   `n_heads`: The number of attention heads used in the multi-head attention mechanism within each transformer layer.\n",
        "*   `d_model`: The dimensionality of the model's embeddings and internal representations.\n",
        "*   `block_size`: The maximum sequence length or context window that the model can process at once.\n",
        "*   `dropout`: The dropout rate applied for regularization to prevent overfitting.\n",
        "\n",
        "By using a `dataclass`, we achieve immutability for the configuration once defined (due to `frozen=True`), which helps prevent accidental changes to the model's blueprint during its lifecycle. The `head_dim` property is also derived to ensure `d_model` is divisible by `n_heads`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75a299e1"
      },
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass(frozen=True)#prevents accidental mutation\n",
        "class ModelConfig:\n",
        "    vocab_size: int\n",
        "    n_layers: int\n",
        "    n_heads: int\n",
        "    d_model: int\n",
        "    block_size: int\n",
        "    dropout: float = 0.0\n",
        "\n",
        "    @property\n",
        "    def head_dim(self) -> int:\n",
        "        return self.d_model // self.n_heads\n",
        "\n",
        "    def __post_init__(self):\n",
        "        assert self.d_model % self.n_heads == 0, \"d_model must be divisible by n_heads\""
      ],
      "execution_count": 343,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a66fe78"
      },
      "source": [
        "### Embedding Layers: Token and Positional\n",
        "\n",
        "Transformer models rely on embedding layers to convert discrete input tokens into continuous vector representations, capturing both semantic meaning and sequential order.\n",
        "\n",
        "#### `TokenEmbedding`\n",
        "\n",
        "This layer converts numerical token IDs into dense vectors. Each unique token in the vocabulary is mapped to a `d_model`-dimensional vector, allowing the model to process linguistic information. This is achieved using `torch.nn.Embedding`, where `vocab_size` determines the number of unique tokens and `d_model` is the dimensionality of the embedding vectors.\n",
        "\n",
        "#### `PositionalEmbedding`\n",
        "\n",
        "Since Transformers process sequences in parallel and lack an inherent understanding of token order, positional embeddings are crucial. This layer provides a vector representation for each position within the input sequence up to `block_size`. These positional vectors are added to the token embeddings, injecting information about the relative or absolute position of each token in the sequence. Like token embeddings, it uses `torch.nn.Embedding` to map position IDs to `d_model`-dimensional vectors.\n",
        "\n",
        "**Key Concept:** The final input to the Transformer encoder is typically the sum of the token embedding and its corresponding positional embedding. This combined representation allows the model to differentiate between identical tokens appearing at different positions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0ddd496"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Assuming ModelConfig is defined in model.config or already imported\n",
        "# from model.config import ModelConfig # Uncomment if not already imported\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=config.vocab_size,\n",
        "            embedding_dim=config.d_model\n",
        "        )\n",
        "\n",
        "    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        token_ids: (B, T)\n",
        "        returns:   (B, T, D)\n",
        "        \"\"\"\n",
        "        return self.embedding(token_ids)\n",
        "\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=config.block_size,\n",
        "            embedding_dim=config.d_model\n",
        "        )\n",
        "\n",
        "    def forward(self, position_ids: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        position_ids: (T) or (B, T)\n",
        "        returns:      (B, T, D)\n",
        "        \"\"\"\n",
        "        return self.embedding(position_ids)"
      ],
      "execution_count": 344,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98a58094"
      },
      "source": [
        "### Scaled Dot-Product Attention\n",
        "\n",
        "Scaled Dot-Product Attention is a fundamental component of the Transformer architecture, designed to efficiently compute attention weights. It takes three inputs: a query matrix (Q), a key matrix (K), and a value matrix (V). The core idea is to calculate a similarity score between the queries and keys, scale these scores, and then use them to weigh the values.\n",
        "\n",
        "**Description:**\n",
        "\n",
        "1.  **Similarity Calculation:** The attention scores are computed by taking the dot product of the query and key matrices. This measures how relevant each key is to each query.\n",
        "2.  **Scaling:** The scores are then divided by the square root of the dimension of the keys (`d_k`). This scaling factor is crucial for preventing the dot products from becoming too large, especially with high `d_k` values, which can push the softmax function into regions with extremely small gradients, hindering training.\n",
        "3.  **Masking (Optional):** If a mask is provided, typically for causality (to prevent attention to future tokens in sequence generation) or padding (to ignore non-existent tokens), the masked positions are set to a very small negative number (e.g., `-inf`). This ensures that after the softmax operation, these positions will have an attention weight of approximately zero.\n",
        "4.  **Softmax:** A softmax function is applied to the scaled scores to obtain attention weights. This normalizes the scores such that they sum to 1, representing a probability distribution over the values.\n",
        "5.  **Weighted Sum:** Finally, these attention weights are multiplied by the value matrix (V). This creates a weighted sum of the values, where the weight assigned to each value is determined by its relevance to the query.\n",
        "\n",
        "**Mathematical Formula:**\n",
        "\n",
        "The Scaled Dot-Product Attention mechanism is mathematically expressed as:\n",
        "\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "Where:\n",
        "-   $Q$ is the Query matrix.\n",
        "-   $K$ is the Key matrix.\n",
        "-   $V$ is the Value matrix.\n",
        "-   $d_k$ is the dimension of the key vectors."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, d_model: int):\n",
        "        super().__init__()\n",
        "        self.scale = 1.0 / math.sqrt(d_model)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        q: torch.Tensor,\n",
        "        k: torch.Tensor,\n",
        "        v: torch.Tensor,\n",
        "        mask: torch.Tensor | None = None\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        q, k, v: (B, T, D)\n",
        "        mask:    (T, T) or (B, T, T)\n",
        "        return:  (B, T, D)\n",
        "        \"\"\"\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        weights = torch.softmax(scores, dim=-1)\n",
        "        output = torch.matmul(weights, v)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "rYFuFZOw_EP9"
      },
      "execution_count": 345,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b4a41f5"
      },
      "source": [
        "### Causal Self-Attention\n",
        "\n",
        "`CausalSelfAttention` is a crucial component in transformer-based autoregressive models, such as GPT (Generative Pre-trained Transformer). It extends the concept of `ScaledDotProductAttention` by ensuring that during sequence generation, each token can only attend to previous tokens and itself, not future tokens. This is vital for tasks like language modeling where predicting the next word depends only on the words that have already occurred.\n",
        "\n",
        "**Description:**\n",
        "\n",
        "1.  **Initialization (`__init__`)**:\n",
        "    *   It takes a `ModelConfig` object, which defines parameters like the number of attention heads (`n_heads`), the dimensionality of each head (`head_dim`), and the model's total dimension (`d_model`).\n",
        "    *   `self.scale`: A scaling factor `1 / sqrt(head_dim)` is calculated, which is standard for scaled dot-product attention to prevent large dot products from pushing the softmax into regions with tiny gradients.\n",
        "    *   `self.qkv_proj`: A linear projection layer that transforms the input `x` (with shape `(B, T, D)`) into Query (Q), Key (K), and Value (V) matrices. It outputs `3 * d_model` dimensions, which are then split into `d_model` for Q, K, and V respectively.\n",
        "    *   `self.out_proj`: Another linear projection layer that takes the concatenated output from all attention heads and projects it back to the original `d_model` dimension.\n",
        "    *   `self.causal_mask`: A lower triangular matrix (e.g., `[[1,0,0],[1,1,0],[1,1,1]]`) is created. This mask is used to block attention to future tokens. It's registered as a buffer, meaning it's part of the model's state but not a trainable parameter.\n",
        "\n",
        "2.  **Forward Pass (`forward`)**:\n",
        "    *   **Input**: `x` with shape `(B, T, D)`, where `B` is batch size, `T` is sequence length, and `D` is `d_model`.\n",
        "    *   **QKV Projections**: The input `x` is passed through `self.qkv_proj` to get a combined `qkv` tensor. This `qkv` tensor is then split into `q`, `k`, and `v` tensors, each of shape `(B, T, D)`.\n",
        "    *   **Multi-Head Reshaping**: Each `q`, `k`, and `v` tensor is reshaped to `(B, n_heads, T, head_dim)`. This involves splitting the `d_model` dimension into `n_heads` separate heads, each with `head_dim` dimensions. The `transpose(1, 2)` operation rearranges the dimensions to put the heads dimension before the sequence length dimension, which is standard for multi-head attention computations.\n",
        "    *   **Attention Scores Calculation**: The core attention mechanism is computed:\n",
        "        $$\\text{scores} = (Q K^T) / \\sqrt{d_k}$$\n",
        "        Here, `q` and `k` (reshaped `(B, n_heads, T, head_dim)`) are multiplied (`torch.matmul`) to get the similarity scores. `k.transpose(-2, -1)` transposes the last two dimensions of `k`, effectively performing $K^T$. The result is then scaled by `self.scale` (`1 / sqrt(head_dim)`).\n",
        "        The shape of `scores` is `(B, n_heads, T, T)`.\n",
        "    *   **Causal Masking**: The `causal_mask` (a lower triangular matrix) is applied. For each position `i` in the sequence, the mask ensures that the attention scores for positions `j > i` (future tokens) are set to negative infinity. This means that after the softmax, these future positions will have an attention weight of zero, effectively preventing a token from attending to future tokens.\n",
        "        `scores = scores.masked_fill(mask == 0, float(\"-inf\"))`\n",
        "    *   **Softmax**: A `softmax` function is applied to the scores along the last dimension (`dim=-1`) to obtain attention `weights`. This normalizes the scores so they sum to 1 for each query, representing a probability distribution over the values.\n",
        "        `weights = torch.softmax(scores, dim=-1)`\n",
        "    *   **Weighted Sum of Values**: The attention `weights` are then multiplied by the `v` (value) tensor (`torch.matmul`). This produces the weighted sum of values, where tokens with higher attention weights contribute more to the output.\n",
        "        `out = torch.matmul(weights, v)`\n",
        "        The shape of `out` is `(B, n_heads, T, head_dim)`.\n",
        "    *   **Merge Heads**: The `out` tensor is reshaped back to `(B, T, D)`. This involves transposing the dimensions back and then concatenating the outputs from all heads (`contiguous().view(B, T, D)`).\n",
        "    *   **Output Projection**: Finally, the merged output is passed through `self.out_proj` to produce the final output of the self-attention layer. This projection allows the model to learn a linear transformation on the combined information from all attention heads.\n",
        "\n",
        "**Mathematical Intuition:**\n",
        "\n",
        "The causal self-attention mechanism fundamentally implements the following operation for each head:\n",
        "\n",
        "$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^T + M}{\\sqrt{d_k}}\\right) V $$\n",
        "\n",
        "Where:\n",
        "*   $Q$, $K$, $V$ are the Query, Key, and Value matrices for a single head.\n",
        "*   $d_k$ is `head_dim`, the dimensionality of the key vectors.\n",
        "*   $M$ is the causal mask, where $M_{ij} = 0$ if $i \\ge j$ (past and current tokens) and $M_{ij} = -\\infty$ if $i < j$ (future tokens). This effectively makes the attention weights to future tokens zero.\n",
        "\n",
        "The multi-head aspect involves performing this attention operation `n_heads` times in parallel with different linear projections for each head, and then concatenating and linearly projecting their outputs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.head_dim = config.head_dim\n",
        "        self.scale = 1.0 / math.sqrt(self.head_dim)\n",
        "\n",
        "        self.qkv_proj = nn.Linear(config.d_model, 3 * config.d_model)\n",
        "        self.out_proj = nn.Linear(config.d_model, config.d_model)\n",
        "\n",
        "        # causal mask (registered as buffer, not parameter)\n",
        "        mask = torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "        self.register_buffer(\"causal_mask\", mask)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: (B, T, D)\n",
        "        return: (B, T, D)\n",
        "        \"\"\"\n",
        "        B, T, D = x.shape\n",
        "\n",
        "        qkv = self.qkv_proj(x)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        # reshape for multi-head\n",
        "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # attention scores\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
        "\n",
        "        # apply causal mask\n",
        "        mask = self.causal_mask[:T, :T]\n",
        "        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
        "\n",
        "        weights = torch.softmax(scores, dim=-1)\n",
        "        out = torch.matmul(weights, v)\n",
        "\n",
        "        # merge heads\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, D)\n",
        "\n",
        "        return self.out_proj(out)\n"
      ],
      "metadata": {
        "id": "-HCKn1q5iagx"
      },
      "execution_count": 346,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9801871b"
      },
      "source": [
        "### KVCache and CachedCausalSelfAttention: Optimizing Inference with KV Caching\n",
        "\n",
        "To enhance the efficiency of autoregressive decoding in Transformer models, especially during inference, Key-Value (KV) caching is employed. This technique avoids redundant re-computation of keys and values for previously processed tokens, significantly speeding up generation. The `KVCache` class manages this storage, and `CachedCausalSelfAttention` utilizes it for incremental token processing.\n",
        "\n",
        "#### 1. `KVCache` Class\n",
        "\n",
        "The `KVCache` class is a simple container designed to store the keys (K) and values (V) computed during the self-attention mechanism across multiple decoding steps. This cache allows subsequent tokens to attend to the full historical context without re-calculating the K and V matrices for past tokens.\n",
        "\n",
        "**Operational Breakdown:**\n",
        "\n",
        "*   **`__init__(self)`**: Initializes an empty cache by setting `self.keys` and `self.values` to `None`. This state indicates that no keys or values have been stored yet.\n",
        "\n",
        "*   **`append(self, k_new: torch.Tensor, v_new: torch.Tensor)`**: This method adds new key and value tensors to the cache. It expects `k_new` and `v_new` to have the shape `(B, H, 1, Dh)`, representing the keys and values for the current token across batches and attention heads.\n",
        "    *   If the cache is empty (`self.keys` is `None`), the new keys and values become the initial content of the cache.\n",
        "    *   If the cache already contains data, `k_new` and `v_new` are concatenated with the existing `self.keys` and `self.values` along the sequence length dimension (dimension 2). This effectively appends the current token's K and V to the historical sequence.\n",
        "\n",
        "*   **`reset(self)`**: Clears the cache by setting `self.keys` and `self.values` back to `None`. This is typically used to prepare the cache for a new generation sequence.\n",
        "\n",
        "#### 2. `CachedCausalSelfAttention` Class\n",
        "\n",
        "The `CachedCausalSelfAttention` module is a specialized version of the `CausalSelfAttention` designed for efficient token-by-token generation (inference) by leveraging the `KVCache`.\n",
        "\n",
        "**Key Differences from `CausalSelfAttention`:**\n",
        "*   It processes input `x_t` with a sequence length `T=1` (a single token at a time).\n",
        "*   It takes a `kv_cache` object as an argument to store and retrieve past keys and values.\n",
        "*   It *implicitly* handles causality by only attending to the `k_full` and `v_full` retrieved from the cache, which by its nature only contains past and current tokens.\n",
        "\n",
        "**Operational Breakdown (`forward` method):**\n",
        "\n",
        "*   **Input**: `x_t` with shape `(B, 1, D)` (a single token per batch) and `kv_cache`.\n",
        "\n",
        "*   **Assertion**: `assert T == 1, \"Cached attention expects exactly one token\"` ensures that the module is used for incremental decoding.\n",
        "\n",
        "*   **QKV Projections**: Similar to `CausalSelfAttention`, `x_t` is projected into query `q`, key `k`, and value `v` tensors for the *current* token.\n",
        "\n",
        "*   **Multi-Head Reshaping**: `q`, `k`, and `v` are reshaped to `(B, n_heads, 1, head_dim)` to prepare for multi-head attention.\n",
        "\n",
        "*   **Cache Append**: The newly computed `k` and `v` for the current token are appended to the `kv_cache` using `kv_cache.append(k, v)`. The cache now holds the keys and values for *all* tokens processed so far in the current sequence.\n",
        "\n",
        "*   **Retrieve Full Cache**: The complete historical `keys` (`k_full`) and `values` (`v_full`) are retrieved from the `kv_cache`. These tensors will have shape `(B, H, T_total, Dh)`, where `T_total` is the current length of the generated sequence.\n",
        "\n",
        "*   **Attention Score Calculation**: The query `q` (current token's query) is used to compute attention scores against `k_full` (all past and current keys). This ensures that the current token attends to the entire context generated so far.\n",
        "    $$\\text{scores} = (Q_{\\text{current}} K_{\\text{full}}^T) / \\sqrt{d_k}$$\n",
        "    The `scores` tensor will have shape `(B, n_heads, 1, T_total)`.\n",
        "\n",
        "*   **Softmax and Weighted Sum**: `softmax` is applied to the scores, and the resulting attention weights are multiplied by `v_full` to produce the output `out` for the current token. This `out` tensor effectively summarizes the information from `v_full` relevant to the current `q`.\n",
        "\n",
        "*   **Merge Heads and Output Projection**: The `out` tensor is reshaped back to `(B, 1, D)` and then passed through `self.out_proj` to yield the final output for the current token.\n",
        "\n",
        "**Mathematical Intuition for Cached Self-Attention:**\n",
        "\n",
        "The core attention computation within `CachedCausalSelfAttention` can be viewed as:\n",
        "\n",
        "$$ \\text{Attention}(\\text{token}_t) = \\text{softmax}\\left(\\frac{Q_t \\cdot K_{\\le t}^T}{\\sqrt{d_k}}\\right) \\cdot V_{\\le t} $$\n",
        "\n",
        "Where:\n",
        "*   $Q_t$ is the Query vector for the current token at position $t$.\n",
        "*   $K_{\\le t}$ represents the concatenated Key matrix containing keys for all tokens from position $1$ up to $t$ (retrieved from `kv_cache.keys`).\n",
        "*   $V_{\\le t}$ represents the concatenated Value matrix containing values for all tokens from position $1$ up to $t$ (retrieved from `kv_cache.values`).\n",
        "*   $d_k$ is the `head_dim`, the dimensionality of the key vectors.\n",
        "\n",
        "In this formulation, the causal masking that explicitly masks future tokens in `CausalSelfAttention` is implicitly handled. By only storing and using keys and values from tokens up to the current position ($K_{\\le t}$ and $V_{\\le t}$), the model naturally prevents attending to future information. The KV cache makes this process highly efficient by avoiding re-computation of $K_{\\le t}$ and $V_{\\le t}$ at each step; instead, it simply appends the new $K_t$ and $V_t$ to the existing cache."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class KVCache:\n",
        "    def __init__(self):\n",
        "        self.keys = None\n",
        "        self.values = None\n",
        "\n",
        "    def append(self, k_new: torch.Tensor, v_new: torch.Tensor):\n",
        "        \"\"\"\n",
        "        k_new, v_new: (B, H, 1, Dh)\n",
        "        \"\"\"\n",
        "        if self.keys is None:\n",
        "            self.keys = k_new\n",
        "            self.values = v_new\n",
        "        else:\n",
        "            self.keys = torch.cat([self.keys, k_new], dim=2)\n",
        "            self.values = torch.cat([self.values, v_new], dim=2)\n",
        "\n",
        "    def reset(self):\n",
        "        self.keys = None\n",
        "        self.values = None\n"
      ],
      "metadata": {
        "id": "jrBBbZLPmWnB"
      },
      "execution_count": 347,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CachedCausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.head_dim = config.head_dim\n",
        "        self.scale = 1.0 / math.sqrt(self.head_dim)\n",
        "\n",
        "        self.qkv_proj = nn.Linear(config.d_model, 3 * config.d_model)\n",
        "        self.out_proj = nn.Linear(config.d_model, config.d_model)\n",
        "\n",
        "    def forward(self, x_t: torch.Tensor, kv_cache) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x_t: (B, 1, D)\n",
        "        kv_cache: KVCache\n",
        "        return: (B, 1, D)\n",
        "        \"\"\"\n",
        "        B, T, D = x_t.shape\n",
        "        assert T == 1, \"Cached attention expects exactly one token\"\n",
        "\n",
        "        qkv = self.qkv_proj(x_t)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        q = q.view(B, 1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(B, 1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(B, 1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # append new K, V to cache\n",
        "        kv_cache.append(k, v)\n",
        "\n",
        "        # retrieve full cached K, V\n",
        "        k_full = kv_cache.keys     # (B, H, T_total, Dh)\n",
        "        v_full = kv_cache.values\n",
        "\n",
        "        scores = torch.matmul(q, k_full.transpose(-2, -1)) * self.scale\n",
        "        weights = torch.softmax(scores, dim=-1)\n",
        "        out = torch.matmul(weights, v_full)\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous().view(B, 1, D)\n",
        "        return self.out_proj(out)\n"
      ],
      "metadata": {
        "id": "G-hcqd0VmZig"
      },
      "execution_count": 348,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.head_dim = config.head_dim\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(config.d_model, 3 * config.d_model)\n",
        "        self.proj = nn.Linear(config.d_model, config.d_model)\n",
        "\n",
        "        mask = torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "        self.register_buffer(\"mask\", mask)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.shape\n",
        "\n",
        "        qkv = self.qkv(x)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        q = q.view(B, T, -1, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(B, T, -1, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(B, T, -1, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        att = att.masked_fill(self.mask[:T, :T] == 0, float(\"-inf\"))\n",
        "        att = torch.softmax(att, dim=-1)\n",
        "\n",
        "        out = att @ v\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, D)\n",
        "        return self.proj(out)\n"
      ],
      "metadata": {
        "id": "mTfaC9ymRm8K"
      },
      "execution_count": 349,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CachedAttentionWrapper(nn.Module):\n",
        "    def __init__(self, attn):\n",
        "        super().__init__()\n",
        "        self.attn = attn   # shared weights\n",
        "\n",
        "    def forward(self, x_t, kv_cache):\n",
        "        \"\"\"\n",
        "        x_t: (B, 1, D)\n",
        "        kv_cache: KVCache\n",
        "        return: (B, 1, D)\n",
        "        \"\"\"\n",
        "        B, T, D = x_t.shape\n",
        "        assert T == 1, \"Cached attention expects exactly one token\"\n",
        "\n",
        "        qkv = self.qkv_proj(x_t)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        q = q.view(B, 1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(B, 1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(B, 1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # append new K, V to cache\n",
        "        kv_cache.append(k, v)\n",
        "\n",
        "        # retrieve full cached K, V\n",
        "        k_full = kv_cache.keys     # (B, H, T_total, Dh)\n",
        "        v_full = kv_cache.values\n",
        "\n",
        "        scores = torch.matmul(q, k_full.transpose(-2, -1)) * self.scale\n",
        "        weights = torch.softmax(scores, dim=-1)\n",
        "        out = torch.matmul(weights, v_full)\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous().view(B, 1, D)\n",
        "        return self.out_proj(out)\n"
      ],
      "metadata": {
        "id": "bdjPpwNOT8F_"
      },
      "execution_count": 350,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.head_dim = config.head_dim\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        self.qkv_proj = nn.Linear(config.d_model, 3 * config.d_model)\n",
        "        self.out_proj = nn.Linear(config.d_model, config.d_model)\n",
        "\n",
        "        mask = torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "        self.register_buffer(\"causal_mask\", mask)\n",
        "\n",
        "    def forward_full(self, x):\n",
        "        B, T, D = x.shape\n",
        "        qkv = self.qkv_proj(x)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        q = q.view(B, T, -1, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(B, T, -1, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(B, T, -1, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        scores = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        mask = self.causal_mask[:T, :T]\n",
        "        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
        "\n",
        "        out = (scores.softmax(-1) @ v)\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, D)\n",
        "        return self.out_proj(out)\n",
        "\n",
        "    def forward_cached(self, x, kv_cache):\n",
        "        B, _, D = x.shape\n",
        "        qkv = self.qkv_proj(x)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        q = q.view(B, 1, -1, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(B, 1, -1, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(B, 1, -1, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        kv_cache.append(k, v)\n",
        "\n",
        "        k_full = kv_cache.keys\n",
        "        v_full = kv_cache.values\n",
        "\n",
        "        scores = (q @ k_full.transpose(-2, -1)) * self.scale\n",
        "        out = (scores.softmax(-1) @ v_full)\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous().view(B, 1, D)\n",
        "        return self.out_proj(out)\n"
      ],
      "metadata": {
        "id": "6R5AjOVyHdJE"
      },
      "execution_count": 351,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc48e58d"
      },
      "source": [
        "### FeedForward Layer\n",
        "\n",
        "In the Transformer architecture, the FeedForward layer (also known as the Position-wise Feed-Forward Network or FFN) is applied independently to each position in the sequence. It consists of two linear transformations with a non-linear activation function (GELU) in between.\n",
        "\n",
        "**Description:**\n",
        "\n",
        "1.  **First Linear Layer (`self.fc1`)**: This layer projects the input `x` from `d_model` dimensions to `4 * d_model` dimensions. This expansion allows the model to learn more complex relationships within each token's representation.\n",
        "2.  **Activation Function (`self.act`)**: A GELU (Gaussian Error Linear Unit) activation function is applied to the output of the first linear layer. GELU is a smooth approximation of the ReLU activation function, often performing better in Transformer-based models.\n",
        "3.  **Second Linear Layer (`self.fc2`)**: This layer projects the expanded representation back from `4 * d_model` dimensions to the original `d_model` dimensions. This ensures that the output of the FFN has the same dimensionality as its input, allowing for residual connections.\n",
        "\n",
        "This layer processes each position identically but independently, meaning the same weights are used for all positions, but each position gets its own distinct computation.\n",
        "\n",
        "**Mathematical Formula:**\n",
        "\n",
        "The FeedForward layer can be mathematically expressed as:\n",
        "\n",
        "$$\\text{FFN}(x) = \\text{GELU}(xW_1 + b_1)W_2 + b_2$$\n",
        "\n",
        "Where:\n",
        "*   $x$ is the input to the FeedForward network, typically the output of the self-attention sub-layer.\n",
        "*   $W_1$ and $b_1$ are the weights and biases of the first linear transformation (from `d_model` to `4 * d_model`).\n",
        "*   $W_2$ and $b_2$ are the weights and biases of the second linear transformation (from `4 * d_model` to `d_model`).\n",
        "*   $\\text{GELU}$ is the Gaussian Error Linear Unit activation function."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(config.d_model, 4 * config.d_model)\n",
        "        self.fc2 = nn.Linear(4 * config.d_model, config.d_model)\n",
        "        self.act = nn.GELU()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.fc2(self.act(self.fc1(x)))"
      ],
      "metadata": {
        "id": "ZkghrdjFmle1"
      },
      "execution_count": 352,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f8202b0"
      },
      "source": [
        "### TransformerBlock\n",
        "\n",
        "The `TransformerBlock` is the core building block of the Transformer's encoder and decoder. This particular implementation appears to be a decoder block designed for incremental inference, as indicated by the use of `CachedCausalSelfAttention` and an input shape `(B, 1, D)`.\n",
        "\n",
        "Each `TransformerBlock` consists of two main sub-layers, each followed by a residual connection and layer normalization:\n",
        "\n",
        "1.  **Cached Causal Self-Attention**: Processes the input `x` to allow it to attend to previous tokens, incorporating the KV-cache for efficient inference.\n",
        "2.  **Feed-Forward Network (FFN)**: Further processes the output of the attention layer through two linear transformations with an activation function.\n",
        "\n",
        "**Description:**\n",
        "\n",
        "*   **`self.ln1` (Layer Normalization)**: Applied before the attention sub-layer. Layer Normalization helps stabilize training by normalizing the inputs to the next layer across the feature dimension. It ensures that the mean and variance of the inputs are consistent.\n",
        "*   **`self.attn` (CachedCausalSelfAttention)**: This is the self-attention mechanism, adapted for efficient autoregressive decoding. It takes the layer-normalized input `self.ln1(x)` and a `kv_cache` (which stores keys and values of previously processed tokens for this specific layer). The output of the attention mechanism is then added to the original input `x` via a residual connection.\n",
        "*   **`self.ln2` (Layer Normalization)**: Applied before the Feed-Forward Network. Similar to `self.ln1`, it normalizes the input to the FFN.\n",
        "*   **`self.mlp` (FeedForward Network)**: This is the position-wise feed-forward network. It takes the layer-normalized output of the attention sub-layer `self.ln2(x)` and processes it. The output of the FFN is then added to the result of the attention sub-layer via another residual connection.\n",
        "\n",
        "**Forward Pass Logic (`forward` method):**\n",
        "\n",
        "The `forward` method implements the following sequence of operations:\n",
        "1.  **Attention Sub-layer**: The input `x` is first normalized by `self.ln1`. This normalized input is then passed to the `self.attn` module along with the `kv_cache` for the current layer. The output of the attention module is added back to the original input `x` (residual connection).\n",
        "    *   `x = x + self.attn(self.ln1(x), kv_cache)`\n",
        "2.  **Feed-Forward Sub-layer**: The result from the attention sub-layer is then normalized by `self.ln2`. This normalized result is passed to the `self.mlp` module. The output of the MLP is added back to the result of the attention sub-layer (another residual connection).\n",
        "    *   `x = x + self.mlp(self.ln2(x))`\n",
        "3.  **Output**: The final result `x` is the output of the Transformer block.\n",
        "\n",
        "**Mathematical Formula:**\n",
        "\n",
        "Let $x$ be the input to the Transformer block, and $x_{\\text{cache}}$ represent the `kv_cache` for the current layer.\n",
        "\n",
        "1.  **Layer Normalization 1**: $x' = \\text{LayerNorm}_1(x)$\n",
        "2.  **Cached Causal Self-Attention**: $x'' = \\text{CachedCausalSelfAttention}(x', x_{\\text{cache}})$\n",
        "3.  **Residual Connection 1**: $x_{\\text{attn}} = x + x''$\n",
        "4.  **Layer Normalization 2**: $x''' = \\text{LayerNorm}_2(x_{\\text{attn}})$\n",
        "5.  **Feed-Forward Network**: $x'''' = \\text{FeedForward}(x''')$\n",
        "6.  **Residual Connection 2**: $x_{\\text{output}} = x_{\\text{attn}} + x''''$\n",
        "\n",
        "Thus, the entire block's operation can be summarized as:\n",
        "\n",
        "$$ \\text{TransformerBlock}(x, x_{\\text{cache}}) = \\text{LayerNorm}_2(x + \\text{CachedCausalSelfAttention}(\\text{LayerNorm}_1(x), x_{\\text{cache}})) + \\text{FeedForward}(\\text{LayerNorm}_2(x + \\text{CachedCausalSelfAttention}(\\text{LayerNorm}_1(x), x_{\\text{cache}}))) $$\n",
        "\n",
        "This structure, often referred to as \"Pre-Normalization\" or \"Pre-LN\" Transformer, applies layer normalization before the self-attention and FFN sub-layers."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.d_model)\n",
        "        self.ln2 = nn.LayerNorm(config.d_model)\n",
        "\n",
        "        self.attn = Attention(config)\n",
        "        self.mlp = FeedForward(config)\n",
        "\n",
        "    def forward_full(self, x):\n",
        "        x = x + self.attn.forward_full(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "    def forward_cached(self, x, kv_cache):\n",
        "        x = x + self.attn.forward_cached(self.ln1(x), kv_cache)\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "bZ2ZH_6xmpig"
      },
      "execution_count": 353,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReferenceTransformerBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.d_model)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln2 = nn.LayerNorm(config.d_model)\n",
        "        self.mlp = FeedForward(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "B327wXNHRqkM"
      },
      "execution_count": 354,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60f5a488"
      },
      "source": [
        "### MiniGPTInferenceModel: The Autoregressive Inference Engine\n",
        "\n",
        "The `MiniGPTInferenceModel` class encapsulates the entire Transformer architecture, specifically designed for efficient autoregressive inference (token-by-token generation). It orchestrates the flow of a single input token through the model, leveraging Key-Value (KV) caching to maintain context from previously generated tokens.\n",
        "\n",
        "**Description:**\n",
        "\n",
        "1.  **Initialization (`__init__`)**:\n",
        "    *   **`self.token_emb`**: An `nn.Embedding` layer that converts input token IDs into dense vector representations. Its size is `(vocab_size, d_model)`.\n",
        "    *   **`self.pos_emb`**: An `nn.Embedding` layer that provides positional information, mapping the current `position` in the sequence to a `d_model`-dimensional vector. Its size is `(block_size, d_model)`.\n",
        "    *   **`self.blocks`**: A `nn.ModuleList` containing a stack of `n_layers` `TransformerBlock` instances. Each `TransformerBlock` is configured to use `CachedCausalSelfAttention`, enabling efficient incremental decoding.\n",
        "    *   **`self.ln_f`**: A final `nn.LayerNorm` applied after the stack of Transformer blocks, normalizing the output features before the final prediction head.\n",
        "    *   **`self.lm_head`**: A linear layer (`nn.Linear`) that projects the `d_model`-dimensional output of the Transformer stack to `vocab_size` dimensions, representing the logits for the next token.\n",
        "    *   **`self.kv_caches`**: A list of `KVCache` objects, one for each `TransformerBlock`, initialized in `reset_cache()`. These caches store the keys and values computed by each layer during the generation process.\n",
        "    *   **`self.position`**: An integer tracking the current position in the sequence being generated, used for positional embeddings.\n",
        "\n",
        "2.  **`reset_cache(self)`**:\n",
        "    *   This method initializes or clears the KV caches for all Transformer layers and resets the `self.position` counter to 0. It should be called at the beginning of a new generation sequence.\n",
        "\n",
        "3.  **`forward_step(self, token_ids: torch.Tensor)`**:\n",
        "    *   This method performs a single forward pass for one token (`T=1`), generating the logits for the next token. It's decorated with `@torch.no_grad()` as it's intended for inference, preventing gradient calculation.\n",
        "    *   **Input**: `token_ids` is a tensor of shape `(B, 1)`, where `B` is the batch size and `1` indicates a single token.\n",
        "    *   **Positional Embedding**: The current `self.position` is used to retrieve the appropriate positional embedding. This position is incremented after each `forward_step`.\n",
        "    *   **Input Embedding**: The input `token_ids` are combined with their corresponding positional embeddings: `x = token_emb(token_ids) + pos_emb(position_ids)`.\n",
        "    *   **Transformer Blocks**: The embedded input `x` is sequentially passed through each `TransformerBlock`. Crucially, each block receives its own `kv_cache`, allowing it to append the current token's keys and values and attend to all prior tokens' cached keys and values.\n",
        "    *   **Final Layer Norm and Head**: After passing through all blocks, the output `x` is normalized by `self.ln_f` and then projected through `self.lm_head` to produce `logits` of shape `(B, vocab_size)`.\n",
        "    *   **Output**: The method returns `logits.squeeze(1)`, which are the probabilities (before softmax) for the next token across the vocabulary.\n",
        "\n",
        "**Mathematical Intuition for `forward_step`:**\n",
        "\n",
        "Let $t$ be the current token position in the sequence. The `forward_step` processes a single token at a time:\n",
        "\n",
        "1.  **Input Embedding**: The input token $w_t$ is first converted into its vector representation, combining token and positional information:\n",
        "    $$ \text{embedding}_t = \text{TokenEmbed}(w_t) + \text{PosEmbed}(t) $$\n",
        "\n",
        "2.  **Transformer Blocks**: The embedded token passes through $L$ Transformer blocks. For each block $l$ from $1$ to $L$:\n",
        "    $$ h_t^{(l)} = \text{TransformerBlock}^{(l)}(h_t^{(l-1)}, \text{KVCache}^{(l)}) $$\n",
        "    where $h_t^{(0)} = \text{embedding}_t$. Each `TransformerBlock` computes its self-attention using $Q_t^{(l)}$ (query for current token) and $K_{\text{cached}}^{(l)}$, $V_{\text{cached}}^{(l)}$ (cached keys and values for all tokens up to $t$).\n",
        "\n",
        "3.  **Final Layer Normalization**: After all blocks, the output is normalized:\n",
        "    $$ h_t^{\text{final}} = \text{LayerNorm}_{\text{final}}(h_t^{(L)}) $$\n",
        "\n",
        "4.  **Language Model Head**: The final representation is projected to vocabulary size to obtain logits for the next token:\n",
        "    $$ \text{logits}_{t+1} = \text{LMHead}(h_t^{\text{final}}) $$\n",
        "    These logits represent the unnormalized probabilities for each token in the vocabulary to be the next token in the sequence."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MiniGPTInferenceModel(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.pos_emb = nn.Embedding(config.block_size, config.d_model)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(config) for _ in range(config.n_layers)\n",
        "        ])\n",
        "\n",
        "        self.ln_f = nn.LayerNorm(config.d_model)\n",
        "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "\n",
        "        self.reset_cache()\n",
        "        self.reference_blocks = nn.ModuleList(\n",
        "    [ReferenceTransformerBlock(config) for _ in range(config.n_layers)]\n",
        ")\n",
        "\n",
        "\n",
        "    def reset_cache(self):\n",
        "        self.kv_caches = [KVCache() for _ in range(self.config.n_layers)]\n",
        "        self.position = 0\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward_step(self, token_ids: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        token_ids: (B, 1)\n",
        "        returns logits: (B, vocab_size)\n",
        "        \"\"\"\n",
        "        B, T = token_ids.shape\n",
        "        assert T == 1, \"Inference model expects exactly one token at a time\"\n",
        "\n",
        "        pos = torch.full((B, 1), self.position, device=token_ids.device)\n",
        "\n",
        "        x = self.token_emb(token_ids) + self.pos_emb(pos)\n",
        "\n",
        "        for block, cache in zip(self.blocks, self.kv_caches):\n",
        "          x = block.forward_cached(x, cache)\n",
        "\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        self.position += 1\n",
        "        return logits.squeeze(1)\n",
        "\n",
        "    #helper function just for testing ::Cached vs Non-Cached Attention Equivalence Test\n",
        "    @torch.no_grad()\n",
        "    def forward_full(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        input_ids: (B, T)\n",
        "        returns logits: (B, T, vocab_size)\n",
        "        \"\"\"\n",
        "        B, T = input_ids.shape\n",
        "        positions = torch.arange(T, device=input_ids.device).unsqueeze(0)\n",
        "\n",
        "        x = self.token_emb(input_ids) + self.pos_emb(positions)\n",
        "\n",
        "        for block in self.blocks:\n",
        "          x = block.forward_full(x)\n",
        "\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        return self.lm_head(x)"
      ],
      "metadata": {
        "id": "jz00E0jK4qTP"
      },
      "execution_count": 355,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3b6c990"
      },
      "source": [
        "### `generate` Function: Autoregressive Text Generation\n",
        "\n",
        "The `generate` function is responsible for orchestrating the autoregressive generation of new tokens given an initial prompt (`input_ids`). It utilizes the `MiniGPTInferenceModel` to predict subsequent tokens one by one, leveraging the KV cache for efficiency and sampling new tokens based on predicted probabilities.\n",
        "\n",
        "**Description:**\n",
        "\n",
        "1.  **Initialization (`model.reset_cache()`)**:\n",
        "    *   Before starting a new generation sequence, the `model.reset_cache()` method is called. This clears all Key-Value (KV) caches in each Transformer layer and resets the positional counter, ensuring a clean state for generating new text.\n",
        "\n",
        "2.  **Feeding Prompt Tokens (`for t in range(input_ids.size(1)):`)**:\n",
        "    *   The `input_ids` (the initial prompt) are fed into the model one token at a time. For each token in the prompt, `model.forward_step()` is called. This populates the KV caches within each Transformer layer with the keys and values corresponding to the prompt tokens. Crucially, during this phase, no new tokens are generated; the model is simply \"reading\" the prompt to build its internal context.\n",
        "\n",
        "3.  **Starting Generation (`token = input_ids[:, -1:]`)**:\n",
        "    *   After processing the prompt, the last token of the prompt is selected as the starting point for actual generation. This `token` will be the first input to `model.forward_step()` during the iterative generation loop.\n",
        "    *   An `outputs` list is initialized with this starting token to collect all generated tokens.\n",
        "\n",
        "4.  **Iterative Token Generation (`for _ in range(max_new_tokens):`)**:\n",
        "    *   The core generation loop runs for `max_new_tokens` iterations, where `max_new_tokens` specifies how many new tokens to generate.\n",
        "    *   **Forward Step**: `logits = model.forward_step(token)`: The current `token` (initially the last prompt token, then each newly generated token) is passed through the `MiniGPTInferenceModel`'s `forward_step`. This returns the logits (unnormalized log-probabilities) for the next possible token across the entire vocabulary.\n",
        "    *   **Temperature Scaling**: `logits = logits / temperature`: The `temperature` parameter controls the randomness of the generation. A higher temperature (e.g., > 1.0) makes the softmax distribution flatter, leading to more diverse but potentially less coherent outputs. A lower temperature (e.g., < 1.0) makes the distribution sharper, leading to more deterministic and focused outputs. `temperature = 1.0` means no change.\n",
        "    *   **Probability Distribution**: `probs = torch.softmax(logits, dim=-1)`: The scaled logits are converted into a probability distribution over the vocabulary using the softmax function. This gives the likelihood of each vocabulary token being the next token.\n",
        "    *   **Token Sampling**: `token = torch.multinomial(probs, num_samples=1)`: A new token is sampled from this probability distribution using `torch.multinomial`. This allows for probabilistic generation, where tokens with higher probabilities are more likely to be chosen, but less likely tokens can also be selected.\n",
        "    *   **Append Output**: `outputs.append(token)`: The newly sampled token is appended to the `outputs` list.\n",
        "\n",
        "5.  **Concatenate Outputs (`return torch.cat(outputs, dim=1)`)**:\n",
        "    *   After `max_new_tokens` have been generated, all collected `outputs` (including the initial prompt's last token and all subsequent generated tokens) are concatenated along dimension 1 to form the complete generated sequence.\n",
        "\n",
        "**Mathematical Intuition for `generate`:**\n",
        "\n",
        "Given an initial prompt $P = (p_1, p_2, \text{...}, p_m)$, the function first initializes the model's state:\n",
        "\n",
        "1.  **Prompt Processing**: The model processes each token $p_i$ of the prompt sequentially:\n",
        "    $$ \text{model.forward_step}(p_i) \text{ for } i=1 \text{ to } m $$\n",
        "    This populates the KV caches within the model based on the prompt's context.\n",
        "\n",
        "2.  **Iterative Generation**: Starting with $w_0 = p_m$ (the last token of the prompt), the function iteratively generates new tokens $w_1, w_2, \text{...}, w_N$ for $N = \text{max_new_tokens}$.\n",
        "    For each step $k$ from $0$ to $N-1$:\n",
        "    *   **Predict Logits**: The model computes logits for the next token given the current token $w_k$ and the accumulated KV cache from previous tokens:\n",
        "        $$ L_{k+1} = \text{model.forward_step}(w_k) $$\n",
        "    *   **Apply Temperature**: Scale the logits by temperature:\n",
        "        $$ L'_{k+1} =\n",
        "rac{L_{k+1}}{\text{temperature}} $$\n",
        "    *   **Compute Probabilities**: Convert scaled logits to probabilities:\n",
        "        $$ P_{k+1} = \text{softmax}(L'_{k+1}) $$\n",
        "    *   **Sample Next Token**: Sample the next token $w_{k+1}$ from the probability distribution $P_{k+1}$:\n",
        "        $$ w_{k+1} \thicksim \text{Multinomial}(P_{k+1}) $$\n",
        "\n",
        "3.  **Final Sequence**: The generated sequence is formed by concatenating the initial prompt tokens and the generated tokens:\n",
        "    $$ \text{Output} = (p_1, \text{...}, p_m, w_1, \text{...}, w_N) $$"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def generate(\n",
        "    model,\n",
        "    input_ids: torch.Tensor,\n",
        "    max_new_tokens: int,\n",
        "    temperature: float = 1.0\n",
        "):\n",
        "    model.reset_cache()\n",
        "\n",
        "    # feed prompt tokens first\n",
        "    for t in range(input_ids.size(1)):\n",
        "        model.forward_step(input_ids[:, t:t+1])\n",
        "\n",
        "    token = input_ids[:, -1:]\n",
        "\n",
        "    outputs = [token]\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        logits = model.forward_step(token)\n",
        "        logits = logits / temperature\n",
        "\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        outputs.append(token)\n",
        "\n",
        "    return torch.cat(outputs, dim=1)\n"
      ],
      "metadata": {
        "id": "l4ukDrFT5ajl"
      },
      "execution_count": 356,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step-by-step validation strategy for a KV-cached Transformer"
      ],
      "metadata": {
        "id": "opUgY25S8Q5H"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0576801f"
      },
      "source": [
        "#### KVCache Validation\n",
        "\n",
        "This code snippet demonstrates the functionality of the `KVCache` class. It initializes a cache and then iteratively appends new key and value tensors, showing how the `keys` tensor grows in the sequence length dimension with each append operation. This confirms that the cache is correctly storing and concatenating past keys and values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e36ef20a",
        "outputId": "07466573-d5ac-4539-aa0f-d3aea2f88643"
      },
      "source": [
        "B, H, Dh = 1, 2, 4\n",
        "cache = KVCache()\n",
        "\n",
        "for t in range(3):\n",
        "    k = torch.randn(B, H, 1, Dh)\n",
        "    v = torch.randn(B, H, 1, Dh)\n",
        "    cache.append(k, v)\n",
        "\n",
        "    print(f\"Step {t}: keys shape =\", cache.keys.shape)\n"
      ],
      "execution_count": 357,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: keys shape = torch.Size([1, 2, 1, 4])\n",
            "Step 1: keys shape = torch.Size([1, 2, 2, 4])\n",
            "Step 2: keys shape = torch.Size([1, 2, 3, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e3cc996"
      },
      "source": [
        "#### Cached Attention (Shape and Determinism) Validation\n",
        "\n",
        "This section validates the `CachedCausalSelfAttention` module, ensuring that its output shape remains consistent `(B, 1, D)` during incremental decoding (processing one token at a time). It also implicitly checks that the internal caching mechanism functions without altering the expected output dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d79f4cb9",
        "outputId": "d5293728-ef48-4694-e482-576a57ed8994"
      },
      "source": [
        "config = ModelConfig(\n",
        "    vocab_size=100,\n",
        "    d_model=8,\n",
        "    n_heads=2,\n",
        "    n_layers=1,\n",
        "    block_size=16\n",
        ")\n",
        "\n",
        "attn = CachedCausalSelfAttention(config)\n",
        "cache = KVCache()\n",
        "\n",
        "x = torch.randn(1, 1, 8)\n",
        "\n",
        "for t in range(3):\n",
        "    y = attn(x, cache)\n",
        "    print(f\"Step {t}: output shape =\", y.shape)\n"
      ],
      "execution_count": 358,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: output shape = torch.Size([1, 1, 8])\n",
            "Step 1: output shape = torch.Size([1, 1, 8])\n",
            "Step 2: output shape = torch.Size([1, 1, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09f76272"
      },
      "source": [
        "#### TransformerBlock (Residual Safety) Validation\n",
        "\n",
        "Here, a single `TransformerBlock` is tested in an incremental fashion. The code verifies that the output `x` maintains its shape `(B, 1, D)` after passing through the block, confirming that residual connections and layer normalizations do not introduce dimensional changes. It also shows that the `kv_cache` associated with this block correctly accumulates keys and values for each step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13817821",
        "outputId": "c5cff6bc-eaae-426d-b102-0489c87a1338"
      },
      "source": [
        "block = TransformerBlock(config)\n",
        "cache = KVCache()\n",
        "\n",
        "x = torch.randn(1, 1, 8)\n",
        "\n",
        "for t in range(3):\n",
        "    x = block.forward_cached(x, cache)\n",
        "    print(f\"Step {t}: x shape =\", x.shape,\n",
        "          \"cache length =\", cache.keys.size(2))\n"
      ],
      "execution_count": 359,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: x shape = torch.Size([1, 1, 8]) cache length = 1\n",
            "Step 1: x shape = torch.Size([1, 1, 8]) cache length = 2\n",
            "Step 2: x shape = torch.Size([1, 1, 8]) cache length = 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cd47ab0"
      },
      "source": [
        "#### Full Model `forward_step` Validation\n",
        "\n",
        "This demonstrates the `forward_step` method of the `MiniGPTInferenceModel`. It feeds a single token at a time and verifies that the model produces logits of the expected shape `(B, vocab_size)` and that the internal `position` counter correctly increments with each step, reflecting the current length of the generated sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceae381a",
        "outputId": "f145a6a7-343a-4c8f-9db3-48cc8551e0a0"
      },
      "source": [
        "model = MiniGPTInferenceModel(config)\n",
        "\n",
        "token = torch.tensor([[5]])\n",
        "\n",
        "for t in range(3):\n",
        "    logits = model.forward_step(token)\n",
        "    print(\n",
        "        f\"Step {t}: logits shape =\",\n",
        "        logits.shape,\n",
        "        \"position =\",\n",
        "        model.position\n",
        "    )\n"
      ],
      "execution_count": 360,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: logits shape = torch.Size([1, 100]) position = 1\n",
            "Step 1: logits shape = torch.Size([1, 100]) position = 2\n",
            "Step 2: logits shape = torch.Size([1, 100]) position = 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ef1b43c"
      },
      "source": [
        "#### Minimal Generation Smoke Test\n",
        "\n",
        "This is a basic test of the `generate` function. It provides a short `input_ids` prompt and requests a few new tokens. The output shape is checked to ensure that the function returns a tensor with the expected batch size and a sequence length that combines the prompt length and the number of generated tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4b1dddd",
        "outputId": "c2d23240-75f5-4ba0-db24-a65099582612"
      },
      "source": [
        "input_ids = torch.tensor([[1, 2, 3]])\n",
        "out = generate(model, input_ids, max_new_tokens=5)\n",
        "print(out.shape)\n"
      ],
      "execution_count": 361,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cached vs Non-Cached Attention Equivalence Test"
      ],
      "metadata": {
        "id": "aZnhgWm-BZFz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74804721"
      },
      "source": [
        "###Equivalence Test\n",
        "\n",
        "The test calculates the maximum absolute difference between the logits produced by the `forward_cached` method (which uses the KV cache for incremental processing) and the `forward_full` method (which processes the entire sequence at once). A very small `max_diff` (typically close to floating-point precision limits, like `e-07` or `e-08`) indicates that the cached and non-cached implementations are functionally equivalent. This confirms that the KV caching mechanism correctly preserves the mathematical output of the self-attention mechanism during autoregressive inference."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-initialize model here to ensure it uses the latest class definition from cell jz00E0jK4qTP\n",
        "# 'config' is expected to be available from an earlier cell (d79f4cb9).\n",
        "model = MiniGPTInferenceModel(config)\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "input_ids = torch.tensor([[1, 2, 3, 4]])\n",
        "\n",
        "# cached decoding\n",
        "model.reset_cache()\n",
        "cached_logits = []\n",
        "\n",
        "for t in range(input_ids.size(1)):\n",
        "    logits = model.forward_step(input_ids[:, t:t+1])\n",
        "    cached_logits.append(logits)\n",
        "\n",
        "cached_logits = torch.stack(cached_logits, dim=1)\n",
        "\n",
        "# full forward\n",
        "full_logits = model.forward_full(input_ids)\n",
        "\n",
        "# compare\n",
        "max_diff = (cached_logits - full_logits).abs().max()\n",
        "print(\"Max difference:\", max_diff.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZz2sjfUC_UY",
        "outputId": "ceedf8d5-43bd-4d96-ce59-81fbc06a4b18"
      },
      "execution_count": 362,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max difference: 2.384185791015625e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1018e165"
      },
      "source": [
        "### Max Difference Output\n",
        "\n",
        "The output `Max difference: 3.5762786865234375e-07` is extremely small. This indicates that the logits produced by the `forward_cached` method (which uses the KV cache for incremental processing) and the `forward_full` method (which processes the entire sequence at once) are practically identical. The tiny difference observed is well within typical floating-point precision errors in numerical computations, confirming that the KV caching mechanism is implemented correctly and maintains functional equivalence with the full forward pass."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Top-k and Top-p (Nucleus) Sampling â€” Quality Control in Generation"
      ],
      "metadata": {
        "id": "W9xk6QXfLnM8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1a1146b"
      },
      "source": [
        "### `top_k_filter` Function\n",
        "\n",
        "This function implements Top-K sampling, a method for controlling the diversity of generated text. It works by retaining only the `k` most probable tokens for consideration at each generation step, and setting the probabilities of all other tokens to zero.\n",
        "\n",
        "**How it works:**\n",
        "1.  `torch.topk(probs, k)`: Identifies the `k` tokens with the highest probabilities and their corresponding indices.\n",
        "2.  `filtered = torch.zeros_like(probs)`: Creates a new tensor of zeros with the same shape as the input probabilities.\n",
        "3.  `filtered.scatter_(1, indices, values)`: Scatters the `values` (the top `k` probabilities) into the `filtered` tensor at the `indices` found by `topk`. All other positions remain zero.\n",
        "4.  `return filtered / filtered.sum(dim=-1, keepdim=True)`: Renormalizes the probabilities of the selected `k` tokens so that they sum to 1. This ensures that the filtered distribution remains a valid probability distribution.\n",
        "\n",
        "**Effect:** Top-K sampling reduces the vocabulary size from which the next token is chosen, making the generation process less prone to selecting very unlikely tokens, thus improving coherence."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def top_k_filter(probs, k):\n",
        "    values, indices = torch.topk(probs, k)\n",
        "    filtered = torch.zeros_like(probs)\n",
        "    filtered.scatter_(1, indices, values)\n",
        "    return filtered / filtered.sum(dim=-1, keepdim=True)\n"
      ],
      "metadata": {
        "id": "K3-Yx2VMMeos"
      },
      "execution_count": 363,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "553a5032"
      },
      "source": [
        "### `top_p_filter` (Nucleus Sampling) Function\n",
        "\n",
        "This function implements Top-P sampling, also known as Nucleus Sampling. This method selects the smallest set of most probable tokens whose cumulative probability exceeds a threshold `p`, and then redistributes the probability mass among only these selected tokens.\n",
        "\n",
        "**How it works:**\n",
        "1.  `sorted_probs, sorted_indices = torch.sort(probs, descending=True, dim=-1)`: Sorts the probabilities in descending order and keeps track of their original indices.\n",
        "2.  `cumulative_probs = torch.cumsum(sorted_probs, dim=-1)`: Calculates the cumulative sum of the sorted probabilities.\n",
        "3.  `mask = cumulative_probs <= p` and `mask[..., 0] = True`: Creates a boolean mask. All tokens whose cumulative probability is less than or equal to `p` are kept (`True`). The first token is always kept to ensure at least one token is always in the nucleus.\n",
        "4.  `filtered_sorted_probs = torch.zeros_like(sorted_probs)`: Initializes a zero tensor for the filtered probabilities.\n",
        "5.  `filtered_sorted_probs[mask] = sorted_probs[mask]`: Assigns the probabilities of the tokens within the nucleus (where `mask` is `True`) to the `filtered_sorted_probs` tensor. All others remain zero.\n",
        "6.  `result_probs.scatter_(dim=-1, index=sorted_indices, src=filtered_sorted_probs)`: Reconstructs the probability distribution in the original token order, placing the filtered probabilities back into their correct positions.\n",
        "7.  `return result_probs / result_probs.sum(dim=-1, keepdim=True)`: Renormalizes the probabilities within the nucleus so they sum to 1.\n",
        "\n",
        "**Effect:** Top-P sampling dynamically adjusts the effective vocabulary size based on the shape of the probability distribution. It helps avoid repetitive text generation while maintaining diversity and relevance."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def top_p_filter(probs, p):\n",
        "    sorted_probs, sorted_indices = torch.sort(probs, descending=True, dim=-1)\n",
        "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "\n",
        "    # Create a mask for elements whose cumulative probability is <= p\n",
        "    mask = cumulative_probs <= p\n",
        "    mask[..., 0] = True  # Always keep at least one token\n",
        "\n",
        "    # Set probabilities of tokens outside the mass p to zero\n",
        "    filtered_sorted_probs = torch.zeros_like(sorted_probs)\n",
        "    # This operation correctly assigns values where mask is True, maintaining 2D shape\n",
        "    filtered_sorted_probs[mask] = sorted_probs[mask]\n",
        "\n",
        "    # Re-order the filtered probabilities back to the original index positions\n",
        "    # using scatter_ with the original sorted_indices (which is 2D)\n",
        "    result_probs = torch.zeros_like(probs)\n",
        "    result_probs.scatter_(dim=-1, index=sorted_indices, src=filtered_sorted_probs)\n",
        "\n",
        "    # Renormalize the filtered probabilities\n",
        "    return result_probs / result_probs.sum(dim=-1, keepdim=True)"
      ],
      "metadata": {
        "id": "IDkb_9aTMeQI"
      },
      "execution_count": 364,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "792bd8b2"
      },
      "source": [
        "### `generate1` Function\n",
        "\n",
        "This is an enhanced version of the `generate` function that incorporates temperature scaling, Top-K, and Top-P (Nucleus) sampling for more flexible and controlled text generation. It processes tokens one by one, leveraging the KV cache for efficiency.\n",
        "\n",
        "**Key Features:**\n",
        "1.  **Cache Reset:** `model.reset_cache()` ensures a clean state for each new generation sequence.\n",
        "2.  **Prompt Processing:** The input `input_ids` (prompt) are fed token by token into `model.forward_step()` to build up the KV cache without generating new tokens.\n",
        "3.  **Iterative Generation:** The loop runs for `max_new_tokens` to generate subsequent tokens.\n",
        "4.  **Greedy Sampling (`temperature = 0.0`):** If `temperature` is 0, the function bypasses probability calculation and directly picks the token with the highest logit using `torch.argmax`. This results in deterministic, greedy generation.\n",
        "5.  **Temperature Scaling:** For `temperature > 0.0`, `logits` are divided by `temperature` before `softmax`. A higher temperature flattens the probability distribution, making less likely tokens more probable and increasing randomness. A lower temperature sharpens the distribution, making the most likely tokens even more probable.\n",
        "6.  **Top-K Filtering:** If `top_k` is provided, `top_k_filter` is applied to prune the probability distribution to only the `k` most probable tokens.\n",
        "7.  **Top-P Filtering:** If `top_p` is provided, `top_p_filter` is applied to select the smallest set of tokens whose cumulative probability exceeds `p`.\n",
        "8.  **Token Sampling:** `torch.multinomial(probs, num_samples=1)` samples a new token based on the (optionally filtered and temperature-scaled) probability distribution.\n",
        "9.  **Output Accumulation:** Each newly generated token is appended to the `outputs` list, which is concatenated at the end to form the complete generated sequence."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def generate1(\n",
        "    model,\n",
        "    input_ids,\n",
        "    max_new_tokens,\n",
        "    temperature=1.0,\n",
        "    top_k=None,\n",
        "    top_p=None\n",
        "):\n",
        "    model.reset_cache()\n",
        "\n",
        "    for t in range(input_ids.size(1)):\n",
        "        model.forward_step(input_ids[:, t:t+1])\n",
        "\n",
        "    token = input_ids[:, -1:]\n",
        "    outputs = [token]\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        logits = model.forward_step(token)\n",
        "\n",
        "        if temperature == 0.0:\n",
        "            # Greedy sampling (argmax)\n",
        "            token = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "        else:\n",
        "            logits = logits / temperature\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "            if top_k is not None:\n",
        "                probs = top_k_filter(probs, top_k)\n",
        "\n",
        "            if top_p is not None:\n",
        "                probs = top_p_filter(probs, top_p)\n",
        "\n",
        "            token = torch.multinomial(probs, num_samples=1)\n",
        "        outputs.append(token)\n",
        "\n",
        "    return torch.cat(outputs, dim=1)"
      ],
      "metadata": {
        "id": "KUsaq8MRMVUU"
      },
      "execution_count": 365,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing Top-k / Top-p Sampling (Correctness + Behavior) Test Case\n",
        "\n",
        "This section contains several test cases designed to validate the correctness and behavior of the `generate1` function, particularly focusing on the `top_k` and `top_p` sampling strategies and the effect of `temperature`."
      ],
      "metadata": {
        "id": "3ASS_agVNKib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Deterministic sanity check (no sampling)\n",
        "**Purpose:** To verify that when `temperature` is set to `0.0`, the `generate1` function behaves deterministically and performs greedy sampling (i.e., always picking the token with the highest probability). By setting a `torch.manual_seed(0)`, we ensure that if there were any randomness, it would be consistent, but with `temperature=0.0`, the output should be identical every time the cell is run with the same initial `input_ids`.\n",
        "\n",
        "**Expected Outcome:** The output sequence of tokens should be the same each time, confirming greedy selection."
      ],
      "metadata": {
        "id": "7frkFNfHNN0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "input_ids = torch.tensor([[1, 2, 3]])\n",
        "\n",
        "out = generate1(\n",
        "    model,\n",
        "    input_ids,\n",
        "    max_new_tokens=5,\n",
        "    temperature=0.0,   # argmax behavior\n",
        "    top_k=None,\n",
        "    top_p=None\n",
        ")\n",
        "\n",
        "print(out)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KXUUPAtMuxB",
        "outputId": "6119723b-e387-48b2-cc9e-f3ad007165d9"
      },
      "execution_count": 366,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 3, 77, 48, 68, 15, 71]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Top-k constraint test (hard cutoff)\n",
        "**Purpose:** To confirm that the `top_k_filter` function correctly identifies and keeps only the `k` most probable tokens, setting all other probabilities to zero, and then renormalizing the remaining probabilities.\n",
        "\n",
        "**Expected Outcome:** The `print` statement should show `Non-zero entries: 5` (or whatever `k` is set to), indicating that only the top `k` probabilities are non-zero after filtering."
      ],
      "metadata": {
        "id": "htKrmArKNRG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "logits = torch.randn(1, model.config.vocab_size)\n",
        "probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "filtered = top_k_filter(probs, k=5)\n",
        "\n",
        "print(\"Non-zero entries:\", (filtered > 0).sum().item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0Er4lYHNS-R",
        "outputId": "0f9e4005-a394-4c23-b131-b80dcd7ebd2f"
      },
      "execution_count": 367,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Non-zero entries: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Top-p constraint test (probability mass)\n",
        "**Purpose:** To verify that the `top_p_filter` function correctly identifies the smallest set of most probable tokens whose cumulative probability exceeds `p`, and then retains only those tokens, setting others to zero, followed by renormalization.\n",
        "\n",
        "**Expected Outcome:** The `print` statement should show a `Cumulative prob` very close to `1.0` (due to renormalization) for the filtered probabilities. This indicates that the filtering and renormalization were performed correctly, maintaining the total probability mass among the selected tokens."
      ],
      "metadata": {
        "id": "HJteWtWYNXSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "logits = torch.randn(1, model.config.vocab_size)\n",
        "probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "filtered = top_p_filter(probs, p=0.9)\n",
        "\n",
        "sorted_probs, _ = torch.sort(filtered, descending=True)\n",
        "print(\"Cumulative prob:\", sorted_probs.sum().item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8J_XLv4Na7H",
        "outputId": "78261dd2-fc3d-4f64-dff7-0a9ac5507bb4"
      },
      "execution_count": 368,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cumulative prob: 0.9999999403953552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Stochasticity test (sampling actually random)\n",
        "**Purpose:** To demonstrate that when `temperature > 0.0` and `top_p` (or `top_k`) filtering is applied, the generation process is indeed stochastic. By setting different random seeds (`torch.manual_seed(42)` and `torch.manual_seed(43)`), we expect to get different generated sequences, even with the same initial prompt and sampling parameters.\n",
        "\n",
        "**Expected Outcome:** `out1` and `out2` should contain different sequences of generated tokens, proving that the sampling mechanism introduces randomness."
      ],
      "metadata": {
        "id": "Z1DCvYDMNsUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "out1 = generate1(model, input_ids, 10, temperature=1.0, top_p=0.9)\n",
        "\n",
        "torch.manual_seed(43)\n",
        "out2 = generate1(model, input_ids, 10, temperature=1.0, top_p=0.9)\n",
        "\n",
        "print(out1)\n",
        "print(out2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRw2FcC-NubE",
        "outputId": "bab3c782-ca2d-48a3-8e8b-00418528bf48"
      },
      "execution_count": 373,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 3, 75, 29, 97, 80, 49, 64,  7, 92, 22,  7]])\n",
            "tensor([[ 3, 18, 10, 22, 27, 50, 77, 45, 13, 17, 92]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Entropy control (temperature effect)\n",
        "**Purpose:** To illustrate how the `temperature` parameter influences the 'creativity' or 'randomness' of the generated text. A lower temperature makes the probability distribution sharper (more likely to pick the most probable token), leading to more deterministic and focused output. A higher temperature flattens the distribution, increasing the chances of picking less probable tokens, resulting in more diverse and potentially 'creative' (but sometimes less coherent) output.\n",
        "\n",
        "**Expected Outcome:** As `temp` increases, the generated sequences are expected to diverge more significantly, demonstrating the increased entropy in sampling."
      ],
      "metadata": {
        "id": "Q9PmUF_sN2iq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for temp in [0.3, 0.7, 1.2]:\n",
        "    torch.manual_seed(0)\n",
        "    out = generate1(\n",
        "        model,\n",
        "        input_ids,\n",
        "        10,\n",
        "        temperature=temp,\n",
        "        top_p=0.9\n",
        "    )\n",
        "    print(f\"Temp={temp} â†’\", out)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAgh084hN5Kt",
        "outputId": "18cbdc6b-745f-4f59-8dc6-64cfb7479d79"
      },
      "execution_count": 372,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temp=0.3 â†’ tensor([[ 3, 77, 15, 51,  8, 82, 86, 40, 92, 55,  7]])\n",
            "Temp=0.7 â†’ tensor([[ 3, 58, 15, 51,  8,  2,  6, 40, 28, 55,  7]])\n",
            "Temp=1.2 â†’ tensor([[ 3, 58, 15, 51,  8,  2,  6, 99, 28, 55,  7]])\n"
          ]
        }
      ]
    }
  ]
}