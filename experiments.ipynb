{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaZd80NcM6uXGn+LijTAsU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Suraj-Sedai/kv-cache-transformer/blob/main/experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MiniGPT-Inference [KV-CACHE TRANSFORMER]\n",
        "\n",
        "**A From-Scratch Transformer Inference Engine**\n",
        "\n",
        "MiniGPT-Inference is a from-scratch, production-grade Transformer inference engine designed to execute autoregressive decoding efficiently using Keyâ€“Value (KV) caching, incremental decoding, and batched generation.\n",
        "\n",
        "Unlike training-focused implementations, this project centers on inference-time systems engineering, emphasizing:\n",
        "- Computational complexity reduction\n",
        "- Memory efficiency\n",
        "- Deterministic correctness\n",
        "- Measurable performance gains\n",
        "\n",
        "The system is architected to reflect how modern large language models (LLMs) are served in real-world environments."
      ],
      "metadata": {
        "id": "bq03RwLpRE-9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48b947b0"
      },
      "source": [
        "# Project Setup: Model Architecture Configuration\n",
        "\n",
        "This section outlines the foundational configuration for our model. The `ModelConfig` dataclass is used to define key architectural hyperparameters, centralizing them for clarity, reusability, and ease of modification.\n",
        "\n",
        "The parameters included in `ModelConfig` are typically found in transformer-based models and include:\n",
        "*   `vocab_size`: The size of the vocabulary, representing the number of unique tokens the model can process.\n",
        "*   `n_layers`: The number of transformer layers or blocks within the model's architecture.\n",
        "*   `n_heads`: The number of attention heads used in the multi-head attention mechanism within each transformer layer.\n",
        "*   `d_model`: The dimensionality of the model's embeddings and internal representations.\n",
        "*   `block_size`: The maximum sequence length or context window that the model can process at once.\n",
        "*   `dropout`: The dropout rate applied for regularization to prevent overfitting.\n",
        "\n",
        "By using a `dataclass`, we achieve immutability for the configuration once defined (due to `frozen=True`), which helps prevent accidental changes to the model's blueprint during its lifecycle. The `head_dim` property is also derived to ensure `d_model` is divisible by `n_heads`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "256e7e4b",
        "outputId": "b6b5ffcc-1726-486d-af7e-153097da187f"
      },
      "source": [
        "# Example: Creating a ModelConfig instance\n",
        "# These are illustrative values and should be adjusted based on your specific model and dataset\n",
        "config = ModelConfig(\n",
        "    vocab_size=10000,\n",
        "    n_layers=6,\n",
        "    n_heads=8,\n",
        "    d_model=512,\n",
        "    block_size=256,\n",
        "    dropout=0.1\n",
        ")\n",
        "\n",
        "print(\"Model Configuration:\")\n",
        "print(f\"  Vocabulary Size: {config.vocab_size}\")\n",
        "print(f\"  Number of Layers: {config.n_layers}\")\n",
        "print(f\"  Number of Attention Heads: {config.n_heads}\")\n",
        "print(f\"  Model Dimension: {config.d_model}\")\n",
        "print(f\"  Block Size (Context Window): {config.block_size}\")\n",
        "print(f\"  Dropout Rate: {config.dropout}\")\n",
        "print(f\"  Head Dimension: {config.head_dim}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Configuration:\n",
            "  Vocabulary Size: 10000\n",
            "  Number of Layers: 6\n",
            "  Number of Attention Heads: 8\n",
            "  Model Dimension: 512\n",
            "  Block Size (Context Window): 256\n",
            "  Dropout Rate: 0.1\n",
            "  Head Dimension: 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75a299e1"
      },
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass(frozen=True)#prevents accidental mutation\n",
        "class ModelConfig:\n",
        "    vocab_size: int\n",
        "    n_layers: int\n",
        "    n_heads: int\n",
        "    d_model: int\n",
        "    block_size: int\n",
        "    dropout: float = 0.0\n",
        "\n",
        "    @property\n",
        "    def head_dim(self) -> int:\n",
        "        return self.d_model // self.n_heads\n",
        "\n",
        "    def __post_init__(self):\n",
        "        assert self.d_model % self.n_heads == 0, \"d_model must be divisible by n_heads\""
      ],
      "execution_count": 6,
      "outputs": []
    }
  ]
}