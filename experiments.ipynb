{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPly5Cw5bXDVvq9La81q9Ww",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Suraj-Sedai/kv-cache-transformer/blob/main/experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MiniGPT-Inference [KV-CACHE TRANSFORMER]\n",
        "\n",
        "**A From-Scratch Transformer Inference Engine**\n",
        "\n",
        "MiniGPT-Inference is a from-scratch, production-grade Transformer inference engine designed to execute autoregressive decoding efficiently using Keyâ€“Value (KV) caching, incremental decoding, and batched generation.\n",
        "\n",
        "Unlike training-focused implementations, this project centers on inference-time systems engineering, emphasizing:\n",
        "- Computational complexity reduction\n",
        "- Memory efficiency\n",
        "- Deterministic correctness\n",
        "- Measurable performance gains\n",
        "\n",
        "The system is architected to reflect how modern large language models (LLMs) are served in real-world environments."
      ],
      "metadata": {
        "id": "bq03RwLpRE-9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48b947b0"
      },
      "source": [
        "# Project Setup: Model Architecture Configuration\n",
        "\n",
        "This section outlines the foundational configuration for our model. The `ModelConfig` dataclass is used to define key architectural hyperparameters, centralizing them for clarity, reusability, and ease of modification.\n",
        "\n",
        "The parameters included in `ModelConfig` are typically found in transformer-based models and include:\n",
        "*   `vocab_size`: The size of the vocabulary, representing the number of unique tokens the model can process.\n",
        "*   `n_layers`: The number of transformer layers or blocks within the model's architecture.\n",
        "*   `n_heads`: The number of attention heads used in the multi-head attention mechanism within each transformer layer.\n",
        "*   `d_model`: The dimensionality of the model's embeddings and internal representations.\n",
        "*   `block_size`: The maximum sequence length or context window that the model can process at once.\n",
        "*   `dropout`: The dropout rate applied for regularization to prevent overfitting.\n",
        "\n",
        "By using a `dataclass`, we achieve immutability for the configuration once defined (due to `frozen=True`), which helps prevent accidental changes to the model's blueprint during its lifecycle. The `head_dim` property is also derived to ensure `d_model` is divisible by `n_heads`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75a299e1"
      },
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass(frozen=True)#prevents accidental mutation\n",
        "class ModelConfig:\n",
        "    vocab_size: int\n",
        "    n_layers: int\n",
        "    n_heads: int\n",
        "    d_model: int\n",
        "    block_size: int\n",
        "    dropout: float = 0.0\n",
        "\n",
        "    @property\n",
        "    def head_dim(self) -> int:\n",
        "        return self.d_model // self.n_heads\n",
        "\n",
        "    def __post_init__(self):\n",
        "        assert self.d_model % self.n_heads == 0, \"d_model must be divisible by n_heads\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a66fe78"
      },
      "source": [
        "### Embedding Layers: Token and Positional\n",
        "\n",
        "Transformer models rely on embedding layers to convert discrete input tokens into continuous vector representations, capturing both semantic meaning and sequential order.\n",
        "\n",
        "#### `TokenEmbedding`\n",
        "\n",
        "This layer converts numerical token IDs into dense vectors. Each unique token in the vocabulary is mapped to a `d_model`-dimensional vector, allowing the model to process linguistic information. This is achieved using `torch.nn.Embedding`, where `vocab_size` determines the number of unique tokens and `d_model` is the dimensionality of the embedding vectors.\n",
        "\n",
        "#### `PositionalEmbedding`\n",
        "\n",
        "Since Transformers process sequences in parallel and lack an inherent understanding of token order, positional embeddings are crucial. This layer provides a vector representation for each position within the input sequence up to `block_size`. These positional vectors are added to the token embeddings, injecting information about the relative or absolute position of each token in the sequence. Like token embeddings, it uses `torch.nn.Embedding` to map position IDs to `d_model`-dimensional vectors.\n",
        "\n",
        "**Key Concept:** The final input to the Transformer encoder is typically the sum of the token embedding and its corresponding positional embedding. This combined representation allows the model to differentiate between identical tokens appearing at different positions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0ddd496"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Assuming ModelConfig is defined in model.config or already imported\n",
        "# from model.config import ModelConfig # Uncomment if not already imported\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=config.vocab_size,\n",
        "            embedding_dim=config.d_model\n",
        "        )\n",
        "\n",
        "    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        token_ids: (B, T)\n",
        "        returns:   (B, T, D)\n",
        "        \"\"\"\n",
        "        return self.embedding(token_ids)\n",
        "\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=config.block_size,\n",
        "            embedding_dim=config.d_model\n",
        "        )\n",
        "\n",
        "    def forward(self, position_ids: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        position_ids: (T) or (B, T)\n",
        "        returns:      (B, T, D)\n",
        "        \"\"\"\n",
        "        return self.embedding(position_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98a58094"
      },
      "source": [
        "### Scaled Dot-Product Attention\n",
        "\n",
        "Scaled Dot-Product Attention is a fundamental component of the Transformer architecture, designed to efficiently compute attention weights. It takes three inputs: a query matrix (Q), a key matrix (K), and a value matrix (V). The core idea is to calculate a similarity score between the queries and keys, scale these scores, and then use them to weigh the values.\n",
        "\n",
        "**Description:**\n",
        "\n",
        "1.  **Similarity Calculation:** The attention scores are computed by taking the dot product of the query and key matrices. This measures how relevant each key is to each query.\n",
        "2.  **Scaling:** The scores are then divided by the square root of the dimension of the keys (`d_k`). This scaling factor is crucial for preventing the dot products from becoming too large, especially with high `d_k` values, which can push the softmax function into regions with extremely small gradients, hindering training.\n",
        "3.  **Masking (Optional):** If a mask is provided, typically for causality (to prevent attention to future tokens in sequence generation) or padding (to ignore non-existent tokens), the masked positions are set to a very small negative number (e.g., `-inf`). This ensures that after the softmax operation, these positions will have an attention weight of approximately zero.\n",
        "4.  **Softmax:** A softmax function is applied to the scaled scores to obtain attention weights. This normalizes the scores such that they sum to 1, representing a probability distribution over the values.\n",
        "5.  **Weighted Sum:** Finally, these attention weights are multiplied by the value matrix (V). This creates a weighted sum of the values, where the weight assigned to each value is determined by its relevance to the query.\n",
        "\n",
        "**Mathematical Formula:**\n",
        "\n",
        "The Scaled Dot-Product Attention mechanism is mathematically expressed as:\n",
        "\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "Where:\n",
        "-   $Q$ is the Query matrix.\n",
        "-   $K$ is the Key matrix.\n",
        "-   $V$ is the Value matrix.\n",
        "-   $d_k$ is the dimension of the key vectors."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, d_model: int):\n",
        "        super().__init__()\n",
        "        self.scale = 1.0 / math.sqrt(d_model)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        q: torch.Tensor,\n",
        "        k: torch.Tensor,\n",
        "        v: torch.Tensor,\n",
        "        mask: torch.Tensor | None = None\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        q, k, v: (B, T, D)\n",
        "        mask:    (T, T) or (B, T, T)\n",
        "        return:  (B, T, D)\n",
        "        \"\"\"\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        weights = torch.softmax(scores, dim=-1)\n",
        "        output = torch.matmul(weights, v)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "rYFuFZOw_EP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b4a41f5"
      },
      "source": [
        "### Causal Self-Attention\n",
        "\n",
        "`CausalSelfAttention` is a crucial component in transformer-based autoregressive models, such as GPT (Generative Pre-trained Transformer). It extends the concept of `ScaledDotProductAttention` by ensuring that during sequence generation, each token can only attend to previous tokens and itself, not future tokens. This is vital for tasks like language modeling where predicting the next word depends only on the words that have already occurred.\n",
        "\n",
        "**Description:**\n",
        "\n",
        "1.  **Initialization (`__init__`)**:\n",
        "    *   It takes a `ModelConfig` object, which defines parameters like the number of attention heads (`n_heads`), the dimensionality of each head (`head_dim`), and the model's total dimension (`d_model`).\n",
        "    *   `self.scale`: A scaling factor `1 / sqrt(head_dim)` is calculated, which is standard for scaled dot-product attention to prevent large dot products from pushing the softmax into regions with tiny gradients.\n",
        "    *   `self.qkv_proj`: A linear projection layer that transforms the input `x` (with shape `(B, T, D)`) into Query (Q), Key (K), and Value (V) matrices. It outputs `3 * d_model` dimensions, which are then split into `d_model` for Q, K, and V respectively.\n",
        "    *   `self.out_proj`: Another linear projection layer that takes the concatenated output from all attention heads and projects it back to the original `d_model` dimension.\n",
        "    *   `self.causal_mask`: A lower triangular matrix (e.g., `[[1,0,0],[1,1,0],[1,1,1]]`) is created. This mask is used to block attention to future tokens. It's registered as a buffer, meaning it's part of the model's state but not a trainable parameter.\n",
        "\n",
        "2.  **Forward Pass (`forward`)**:\n",
        "    *   **Input**: `x` with shape `(B, T, D)`, where `B` is batch size, `T` is sequence length, and `D` is `d_model`.\n",
        "    *   **QKV Projections**: The input `x` is passed through `self.qkv_proj` to get a combined `qkv` tensor. This `qkv` tensor is then split into `q`, `k`, and `v` tensors, each of shape `(B, T, D)`.\n",
        "    *   **Multi-Head Reshaping**: Each `q`, `k`, and `v` tensor is reshaped to `(B, n_heads, T, head_dim)`. This involves splitting the `d_model` dimension into `n_heads` separate heads, each with `head_dim` dimensions. The `transpose(1, 2)` operation rearranges the dimensions to put the heads dimension before the sequence length dimension, which is standard for multi-head attention computations.\n",
        "    *   **Attention Scores Calculation**: The core attention mechanism is computed:\n",
        "        $$\\text{scores} = (Q K^T) / \\sqrt{d_k}$$\n",
        "        Here, `q` and `k` (reshaped `(B, n_heads, T, head_dim)`) are multiplied (`torch.matmul`) to get the similarity scores. `k.transpose(-2, -1)` transposes the last two dimensions of `k`, effectively performing $K^T$. The result is then scaled by `self.scale` (`1 / sqrt(head_dim)`).\n",
        "        The shape of `scores` is `(B, n_heads, T, T)`.\n",
        "    *   **Causal Masking**: The `causal_mask` (a lower triangular matrix) is applied. For each position `i` in the sequence, the mask ensures that the attention scores for positions `j > i` (future tokens) are set to negative infinity. This means that after the softmax, these future positions will have an attention weight of zero, effectively preventing a token from attending to future tokens.\n",
        "        `scores = scores.masked_fill(mask == 0, float(\"-inf\"))`\n",
        "    *   **Softmax**: A `softmax` function is applied to the scores along the last dimension (`dim=-1`) to obtain attention `weights`. This normalizes the scores so they sum to 1 for each query, representing a probability distribution over the values.\n",
        "        `weights = torch.softmax(scores, dim=-1)`\n",
        "    *   **Weighted Sum of Values**: The attention `weights` are then multiplied by the `v` (value) tensor (`torch.matmul`). This produces the weighted sum of values, where tokens with higher attention weights contribute more to the output.\n",
        "        `out = torch.matmul(weights, v)`\n",
        "        The shape of `out` is `(B, n_heads, T, head_dim)`.\n",
        "    *   **Merge Heads**: The `out` tensor is reshaped back to `(B, T, D)`. This involves transposing the dimensions back and then concatenating the outputs from all heads (`contiguous().view(B, T, D)`).\n",
        "    *   **Output Projection**: Finally, the merged output is passed through `self.out_proj` to produce the final output of the self-attention layer. This projection allows the model to learn a linear transformation on the combined information from all attention heads.\n",
        "\n",
        "**Mathematical Intuition:**\n",
        "\n",
        "The causal self-attention mechanism fundamentally implements the following operation for each head:\n",
        "\n",
        "$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^T + M}{\\sqrt{d_k}}\\right) V $$\n",
        "\n",
        "Where:\n",
        "*   $Q$, $K$, $V$ are the Query, Key, and Value matrices for a single head.\n",
        "*   $d_k$ is `head_dim`, the dimensionality of the key vectors.\n",
        "*   $M$ is the causal mask, where $M_{ij} = 0$ if $i \\ge j$ (past and current tokens) and $M_{ij} = -\\infty$ if $i < j$ (future tokens). This effectively makes the attention weights to future tokens zero.\n",
        "\n",
        "The multi-head aspect involves performing this attention operation `n_heads` times in parallel with different linear projections for each head, and then concatenating and linearly projecting their outputs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.head_dim = config.head_dim\n",
        "        self.scale = 1.0 / math.sqrt(self.head_dim)\n",
        "\n",
        "        self.qkv_proj = nn.Linear(config.d_model, 3 * config.d_model)\n",
        "        self.out_proj = nn.Linear(config.d_model, config.d_model)\n",
        "\n",
        "        # causal mask (registered as buffer, not parameter)\n",
        "        mask = torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "        self.register_buffer(\"causal_mask\", mask)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: (B, T, D)\n",
        "        return: (B, T, D)\n",
        "        \"\"\"\n",
        "        B, T, D = x.shape\n",
        "\n",
        "        qkv = self.qkv_proj(x)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        # reshape for multi-head\n",
        "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # attention scores\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
        "\n",
        "        # apply causal mask\n",
        "        mask = self.causal_mask[:T, :T]\n",
        "        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
        "\n",
        "        weights = torch.softmax(scores, dim=-1)\n",
        "        out = torch.matmul(weights, v)\n",
        "\n",
        "        # merge heads\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, D)\n",
        "\n",
        "        return self.out_proj(out)\n",
        ""
      ],
      "metadata": {
        "id": "-HCKn1q5iagx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}